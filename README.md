[![License: CC BY 4.0](https://img.shields.io/badge/License-CC--BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


üåçüìö‚öôÔ∏è  
**Etymologyneering **  
**What if you could learn English (or other languages) by understanding how words are engineered?**

# **Etymologyneering**

An educational experiment combining **mental-models thinking**, **Python**, and **Large Language Models (LLMs)** through APIs that teaches **1,056 English words** through **38 Proto-Indo-European root stems**.

---

## **Concept**

Etymologyneering teaches English through **first principles** i.e. the **Proto-Indo-European (PIE) roots** that act as the atomic units or **bedrock of meaning**. See here about Indo-European reconstructed 
language https://en.wikipedia.org/wiki/Proto-Indo-European_language 
These roots are few, yet they generate thousands of words which feels like a **compression of semantic space**. I mean that each PIE root captures a core meaning (like ‚Äúto carry,‚Äù ‚Äúto shine,‚Äù ‚Äúto bind‚Äù)
and then these few hundred roots generate thousands of words across European Languages (Greek, Latin , Germanic, Balto-Slavic etc including sanskrit language) through systematic transformations (prefixes, suffixes, metaphorical shifts).


Using **analogical thinking**, to me PIE roots mirror how ML models compress data: surface complexity emerging from simpler underlying features.
However their nature is different:
- **Dimensionality reduction** is **computed**: algorithmic extraction of latent features from observed data
- **PIE roots** are **reconstructed**: ancestral forms of an ancient and unwritten language inferred through comparative linguistics 

---

## **Sources**

This work builds upon publicly available etymological data from:

- [**Etymonline.com**](https://www.etymonline.com)  
- [**Wiktionary.org**](https://www.wiktionary.org)

---

## **LLM Workflow**

The **OpenAI ChatGPT-4o API** generated structured entries with the following sections:

- **Greek Translation**  Greek translation for Greek learners
- **Phonetic Spelling**   Provides pronunciation guidance (IPA format)
- **Part of Speech**  e.g., noun, verb, adjective.  
- **Etymology**  historical origin and linguistic evolution.  
- **Nowadays Meaning**  the modern sense.  
- **Connection to the PIE Root Stem**  
  - **Literal:** How concrete/physical meanings evolved into abstract concepts  
  - **Interplay:** How prefix, root, and suffix fuse into meaning.  
- **Example Sentences**  three per entry.  
- **Conclusion** ‚Äì summarizes the evolution from root to modern meaning.  
- **One-Line Intuitive Link** memorable mnemonic compressing the entire etymology.

---

## **Preventing LLM Hallucinations**

A **man-in-the-loop** review process ensured factual grounding and interpretive depth, supported by structured **guardrails**:

- **Missing PIE input for prefixes or stems can lead to hallucinations:** When the model isn‚Äôt provided with the Proto-Indo-European (PIE) root of one or more components (e.g., in a word with two prefixes and one stem),
 it tends to invent or distort meanings for the missing parts. So having all PIE stems for all parts of the words (prefix and stem) is imperative.
- **Schema constraints:** fixed 9-section format with exact headings.  
- **Content separation:** no mixing of historical and modern meaning.  
- **Style rules:** concise sentences (‚â§ 20 words), academic tone.  
- **Evidence discipline:** expand abbreviations; trace prefixes to PIE.  
- **Reasoning scaffolds:** mandatory *Literal ‚Üí Metaphorical Bridge*; compact PIE ‚Üí Modern recap.  
- **Output consistency:** one-line summary ‚â§ 25 words.

---

## **Image Generation Pipeline**

Prompts for **text-to-image generation** were created using **Meta-LLaMA-4-Scout-17b-16e-Instruct** via **Groq API** (the inference accelerator).  

Images were generated by **Black-Forest-Labs / FLUX.1-Schnell** model through the **Hugging Face API**.  
During the review phase of the books, some images were updated directly using **ChatGPT-5** through the Chat UI.  

All captions were LLM-generated, occasionally weirdly funny to me and intentionally left unedited, either through the model  **Meta-LLaMA-4-Scout-17b-16e-Instruct** or 
during my review of the books using  **ChatGPT-5** through the Chat UI.  

---

## **Python Components**

- **BeautifulSoup** ‚Äì scraping etymological data.  
- **Matplotlib** ‚Äì visualizing **clusters of derivative words** around each PIE stem.

---

## **How to Read the Volumes**

For **fast learners** 
1. Speed-read the "PIE Root Connection" section (Literal + Interplay)
2. Glance at the image + 3 example sentences
3. Check the cluster diagram
4. Move to next word

For **deep learners**

1.**Speed-read** each entry.
2. Return to the **cluster diagram**.  
3. Reconnect each word‚Äôs meaning to its **root** and **prefix‚Äìstem interplay**.

Etymologyneering reveals how words are engineered - the **mechanics of meaning**.

---

## **Why Clusters and Images?**

### **Clusters**

Neuroscience suggests the brain stores knowledge as **networks**, not lists.  
Each PIE root sits at the center of a **branching cluster**, mirroring this associative structure.  
This visual hierarchy reflects how ideas interconnect, making comprehension intuitive.

### **Images**

Based on **Dual-Coding Theory (Paivio)** ‚Äî the mind processes **verbal** and **visual** data through two linked systems.  
Combining both enhances **memory retention** and **understanding**, building stronger neural connections than words alone.

---

## **Collaboration Invitation**

If you know of **open etymological datasets or APIs** with accessible word stems and historical explanations (free of licensing restrictions), please DM me as it would be ideal to build an app to teach languages this way.

Future directions for me would be to expand to other domains of foundational knowledge  such as **mathematics**, **code**  etc to explore how mental models thinking,Python,LLMs and collaboration can accelerate studying in other domains.

---

### **Key Takeaways**

- **Etymologyneering is a shortcut to learning foundational knowledge.**  
- **PIE roots = first principles i.e. the atoms of meaning.**  
- **Compression of semantic space** parallels **dimensionality reduction.**  
- **Man-in-the-loop guardrails ensure reliability.**  
- **Clusters mirror how the brain stores knowledge as networks, not lists.**  
- **Dual-coding theory proves that images strengthen memory and comprehension.**

---


**Author:** **Pantelis (L√©on) Ladopoulos**  
**Project Type:** LLM-assisted etymological english learning 
**Languages & Tools:** Python | OpenAI API |Meta Llama API| Groq | Hugging Face API| Matplotlib | BeautifulSoup  

## üìò Download Volumes

You can read or download the **Etymologyneering volumes (PDF)** below.  
Each volume explores English words derived from Proto-Indo-European (PIE) roots,  
featuring imagery, semantic clusters, and explanations showing how prefixes and stems fuse into meaning.

- [üìó Letter L ‚Äî Etymologyneering_EN_L (PDF)](https://github.com/pladopoulos/etymologyneering/raw/main/volumes/Etymologyneering_EN_Letter_L.pdf)
- [üìò Letter U ‚Äî Etymologyneering_EN_U (PDF)](https://github.com/pladopoulos/etymologyneering/raw/main/volumes/Etymologyneering_EN_Letter_U.pdf))
- [üìï Letter W ‚Äî Etymologyneering_EN_W (PDF)](https://github.com/pladopoulos/etymologyneering/raw/main/volumes/Etymologyneering_EN_Letter_W.pdf)
- [üìô Letter Y ‚Äî Etymologyneering_EN_Y (PDF)](https://github.com/pladopoulos/etymologyneering/raw/main/volumes/Etymologyneering_EN_Letter_Y.pdf)



## **License**

- **Books, Text, and Images:** [CC BY 4.0 License](LICENSE_BOOK.txt)
-   You are free to share and adapt this material with proper credit to **Pantelis Ladopoulos**.  

- **Code:** [MIT License](LICENSE_CODE.txt)
-   Free to use, modify, and distribute with attribution.  


